{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Estimasi PI dengan Monte Carlo </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah koding yang digunkana untuk melakukan estimasi nilai PI menggunakan metode monte carlo. Metode Monte Carlo untuk estimasi nilai PI bekerja dengan mencari rasio antara titik data yang berada dalam sebuah lingkaran dengan titik data yang berada di luar lingkaran. Semakin banyak titik data yang dibangkitkan secara acak maka estimasi nilai PI semakin akurat.\n",
    "<br>\n",
    "Pernjelasan Video: https://www.youtube.com/watch?v=ELetCV_wX_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> File Mapper </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah code pada mapper yang digunakan untuk melakukan transformasi data.\n",
    "\"(x1, y1) (x2, y2) (x3, y3) menjadi (didalam) (diluar) (didalam)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting piMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file piMapper.py\n",
    "#!/home/ubuntu/anaconda3/bin/python\n",
    "\n",
    "#Pastikan shebang di atas mengarah ke python interpreter anda.\n",
    "import sys\n",
    "\"\"\"\n",
    "Analisa kode berikut untuk menambahkan pemahaman anda!\n",
    "\"\"\"\n",
    "for value in sys.stdin:\n",
    "    x , y = list(map(float, value.split()))\n",
    "    result = 1 if x*x + y*y <= 1 else 0\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> File Reducer. </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah code pada mapper yang digunakan untuk menghitung estimasi PI menggunakan metode monte carlo.\n",
    "(didalam) (diluar) (didalam) menjadi (estimasi PI) <br><br>\n",
    "Rumus:<br>\n",
    "4 * jumlah_titik_dalam_lingkaran / jumlah_data_keseluruhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting piReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file piReducer.py\n",
    "#!/home/ubuntu/anaconda3/bin/python\n",
    "\n",
    "#Pastikan shebang di atas mengarah ke python interpreter anda.\n",
    "import sys\n",
    "\"\"\"\n",
    "Analisa kode berikut untuk menambahkan pemahaman anda!\n",
    "\"\"\"\n",
    "current_sum = 0\n",
    "count = 0\n",
    "for value in sys.stdin:\n",
    "    current_sum += int(value)\n",
    "    count +=1\n",
    "print(4 * current_sum / count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Berikan izin untuk eksekusi file</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod u+x piMapper.py piReducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Bangkitkan data secara acak </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kode berikut digunakan untuk membangkitkan 10000000 angka acak dari -1 hingga 1\n",
    "#angka yang dihasilkan berupa koordinat <x, y>\n",
    "import random\n",
    "count = 10000000\n",
    "with open(\"random.txt\", \"w\") as numbers:\n",
    "    for i in range(count):\n",
    "        numbers.write(\"%f %f\\n\" % (random.uniform(-1,1), random.uniform(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Menjalankan program tanpa Hadoop </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13868\r\n"
     ]
    }
   ],
   "source": [
    "!cat random.txt | python piMapper.py  | python piReducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Persiapan menjakan dengan Hadoop </h2>\n",
    "-bersihkan input sebelumnya<br>\n",
    "-bersihkan output sebelumnya<br>\n",
    "-buat direktori input<br>\n",
    "-unggah input ke hdfs<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted input/pi\n",
      "rm: `output/pi': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r input/pi\n",
    "!hdfs dfs -rm -r output/pi\n",
    "!hdfs dfs -mkdir -p input/pi\n",
    "!hdfs dfs -put random.txt input/pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Jalankan program menggunakan Hadoop MapReduce </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/06/29 09:00:49 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "19/06/29 09:00:49 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "19/06/29 09:00:49 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "19/06/29 09:00:49 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "19/06/29 09:00:49 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "19/06/29 09:00:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1571049563_0001\n",
      "19/06/29 09:00:50 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "19/06/29 09:00:50 INFO mapreduce.Job: Running job: job_local1571049563_0001\n",
      "19/06/29 09:00:50 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "19/06/29 09:00:50 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "19/06/29 09:00:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/06/29 09:00:50 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/06/29 09:00:50 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "19/06/29 09:00:50 INFO mapred.LocalJobRunner: Starting task: attempt_local1571049563_0001_m_000000_0\n",
      "19/06/29 09:00:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/06/29 09:00:50 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/06/29 09:00:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/06/29 09:00:50 INFO mapred.MapTask: Processing split: hdfs://localhost:8020/user/ubuntu/input/pi/random.txt:0+1800000\n",
      "19/06/29 09:00:50 INFO mapred.MapTask: numReduceTasks: 1\n",
      "19/06/29 09:00:50 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "19/06/29 09:00:50 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "19/06/29 09:00:50 INFO mapred.MapTask: soft limit at 83886080\n",
      "19/06/29 09:00:50 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "19/06/29 09:00:50 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "19/06/29 09:00:50 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "19/06/29 09:00:50 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/notebook/pi/./piMapper.py]\n",
      "19/06/29 09:00:50 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "19/06/29 09:00:50 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "19/06/29 09:00:50 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "19/06/29 09:00:50 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "19/06/29 09:00:50 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "19/06/29 09:00:50 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "19/06/29 09:00:50 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "19/06/29 09:00:50 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "19/06/29 09:00:50 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "19/06/29 09:00:50 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "19/06/29 09:00:50 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "19/06/29 09:00:50 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "19/06/29 09:00:51 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 09:00:51 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 09:00:51 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 09:00:51 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 09:00:51 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 09:00:51 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "19/06/29 09:00:51 INFO mapreduce.Job: Job job_local1571049563_0001 running in uber mode : false\n",
      "19/06/29 09:00:51 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/06/29 09:00:52 INFO streaming.PipeMapRed: R/W/S=100000/90123/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 09:00:52 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/06/29 09:00:52 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/06/29 09:00:52 INFO mapred.LocalJobRunner: \n",
      "19/06/29 09:00:52 INFO mapred.MapTask: Starting flush of map output\n",
      "19/06/29 09:00:52 INFO mapred.MapTask: Spilling map output\n",
      "19/06/29 09:00:52 INFO mapred.MapTask: bufstart = 0; bufend = 300000; bufvoid = 104857600\n",
      "19/06/29 09:00:52 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25814400(103257600); length = 399997/6553600\n",
      "19/06/29 09:00:52 INFO mapred.MapTask: Finished spill 0\n",
      "19/06/29 09:00:52 INFO mapred.Task: Task:attempt_local1571049563_0001_m_000000_0 is done. And is in the process of committing\n",
      "19/06/29 09:00:52 INFO mapred.LocalJobRunner: Records R/W=10000/1\n",
      "19/06/29 09:00:52 INFO mapred.Task: Task 'attempt_local1571049563_0001_m_000000_0' done.\n",
      "19/06/29 09:00:52 INFO mapred.Task: Final Counters for attempt_local1571049563_0001_m_000000_0: Counters: 22\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=134045\n",
      "\t\tFILE: Number of bytes written=999001\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1800000\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=5\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=1\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100000\n",
      "\t\tMap output records=100000\n",
      "\t\tMap output bytes=300000\n",
      "\t\tMap output materialized bytes=500006\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=100000\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=43\n",
      "\t\tTotal committed heap usage (bytes)=121180160\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1800000\n",
      "19/06/29 09:00:52 INFO mapred.LocalJobRunner: Finishing task: attempt_local1571049563_0001_m_000000_0\n",
      "19/06/29 09:00:52 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "19/06/29 09:00:52 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "19/06/29 09:00:52 INFO mapred.LocalJobRunner: Starting task: attempt_local1571049563_0001_r_000000_0\n",
      "19/06/29 09:00:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/06/29 09:00:52 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/06/29 09:00:52 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/06/29 09:00:52 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4f10d8d7\n",
      "19/06/29 09:00:52 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "19/06/29 09:00:52 INFO reduce.EventFetcher: attempt_local1571049563_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "19/06/29 09:00:52 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/06/29 09:00:52 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1571049563_0001_m_000000_0 decomp: 500002 len: 500006 to MEMORY\n",
      "19/06/29 09:00:52 INFO reduce.InMemoryMapOutput: Read 500002 bytes from map-output for attempt_local1571049563_0001_m_000000_0\n",
      "19/06/29 09:00:52 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 500002, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->500002\n",
      "19/06/29 09:00:52 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "19/06/29 09:00:52 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/06/29 09:00:52 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "19/06/29 09:00:52 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/06/29 09:00:52 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 499998 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/06/29 09:00:52 INFO reduce.MergeManagerImpl: Merged 1 segments, 500002 bytes to disk to satisfy reduce memory limit\n",
      "19/06/29 09:00:52 INFO reduce.MergeManagerImpl: Merging 1 files, 500006 bytes from disk\n",
      "19/06/29 09:00:52 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "19/06/29 09:00:52 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/06/29 09:00:52 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 499998 bytes\n",
      "19/06/29 09:00:52 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/06/29 09:00:52 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/notebook/pi/./piReducer.py]\n",
      "19/06/29 09:00:52 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/06/29 09:00:52 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "19/06/29 09:00:52 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 09:00:52 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 09:00:52 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 09:00:52 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 09:00:52 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 09:00:53 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 09:00:53 INFO streaming.PipeMapRed: Records R/W=100000/1\n",
      "19/06/29 09:00:53 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/06/29 09:00:53 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/06/29 09:00:53 INFO mapred.Task: Task:attempt_local1571049563_0001_r_000000_0 is done. And is in the process of committing\n",
      "19/06/29 09:00:53 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/06/29 09:00:53 INFO mapred.Task: Task attempt_local1571049563_0001_r_000000_0 is allowed to commit now\n",
      "19/06/29 09:00:53 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1571049563_0001_r_000000_0' to hdfs://localhost:8020/user/ubuntu/output/pi/_temporary/0/task_local1571049563_0001_r_000000\n",
      "19/06/29 09:00:53 INFO mapred.LocalJobRunner: Records R/W=100000/1 > reduce\n",
      "19/06/29 09:00:53 INFO mapred.Task: Task 'attempt_local1571049563_0001_r_000000_0' done.\n",
      "19/06/29 09:00:53 INFO mapred.Task: Final Counters for attempt_local1571049563_0001_r_000000_0: Counters: 29\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1134089\n",
      "\t\tFILE: Number of bytes written=1499007\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1800000\n",
      "\t\tHDFS: Number of bytes written=9\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=500006\n",
      "\t\tReduce input records=100000\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=100000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=14\n",
      "\t\tTotal committed heap usage (bytes)=121180160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9\n",
      "19/06/29 09:00:53 INFO mapred.LocalJobRunner: Finishing task: attempt_local1571049563_0001_r_000000_0\n",
      "19/06/29 09:00:53 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "19/06/29 09:00:53 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/06/29 09:00:53 INFO mapreduce.Job: Job job_local1571049563_0001 completed successfully\n",
      "19/06/29 09:00:53 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1268134\n",
      "\t\tFILE: Number of bytes written=2498008\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3600000\n",
      "\t\tHDFS: Number of bytes written=9\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100000\n",
      "\t\tMap output records=100000\n",
      "\t\tMap output bytes=300000\n",
      "\t\tMap output materialized bytes=500006\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=500006\n",
      "\t\tReduce input records=100000\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=200000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=57\n",
      "\t\tTotal committed heap usage (bytes)=242360320\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1800000\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9\n",
      "19/06/29 09:00:53 INFO streaming.StreamJob: Output directory: output/pi\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /home/ubuntu/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar \\\n",
    "-mapper piMapper.py \\\n",
    "-reducer piReducer.py \\\n",
    "-input input/pi/random.txt \\\n",
    "-output output/pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Tampilkan output </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13868\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat output/pi/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
