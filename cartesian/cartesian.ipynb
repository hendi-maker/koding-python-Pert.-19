{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting counterMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file counterMapper.py\n",
    "#!/home/ubuntu/anaconda3/bin/python\n",
    "\"\"\"\n",
    "Mapper menghitung banyak data\n",
    "Berikut adalah code pada mapper yang sekaligus berfungsi sebagai kombiner yang digunakan untuk melakukan transformasi data.\n",
    "<data1> <data2> <data3> menjadi <sum(1, 1, 1)>\n",
    "\"\"\"\n",
    "import sys\n",
    "\"\"\"\n",
    "Analisa kode berikut untuk menambahkan pemahaman anda!\n",
    "\n",
    "Mapper yang dibuat terlihat telah menghitung jumlah data tanpa adanya reducer.\n",
    "Akan tetapi, Cobalah jalankan mapper tanpa reducer di bawah untuk menambahkan pemahaman anda tentang MapReduce.\n",
    "\"\"\"\n",
    "counter = 0\n",
    "for line in sys.stdin:\n",
    "    counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting counterReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file counterReducer.py\n",
    "#!/home/ubuntu/anaconda3/bin/python\n",
    "\"\"\"\n",
    "Reducer Menghitung banyak data\n",
    "Berikut adalah code pada reducer untuk menjumlahkan hasil mapper.\n",
    "<sum(jumlah_mapper_1)> <sum(jumlah_mapper_2)> menjadi <sum(jumlah_mapper_1, jumlah_mapper_2)>\n",
    "\"\"\"\n",
    "import sys\n",
    "\"\"\"\n",
    "Analisa kode berikut untuk menambahkan pemahaman anda!\n",
    "\"\"\"\n",
    "current_sum=0\n",
    "for line in sys.stdin:\n",
    "    current_sum += int(line)\n",
    "print(current_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cartesianMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file cartesianMapper.py\n",
    "#!/home/ubuntu/anaconda3/bin/python\n",
    "\"\"\"\n",
    "Mapper Kartesian\n",
    "Berikut adalah code pada mapper yang berfungsi mentransformasikan data.\n",
    "\n",
    "<data1> <data2> <data3> \n",
    "menjadi \n",
    "<(1,1) data1> <(1,2) data1> ... <(1,n) data1> <(2, 1) data1> ... <(n,n) data1> <(1,1) data2> ... <(n,n) datan>\n",
    "\n",
    "\"\"\"\n",
    "import sys\n",
    "\n",
    "#membaca file counter yang berisi nilai banyak data\n",
    "with open(\"counter\",'r') as c:\n",
    "    data_count = int(c.read())\n",
    "\n",
    "\"\"\"\n",
    "Analisa kode berikut untuk menambahkan pemahaman anda!\n",
    "\"\"\"\n",
    "counter = 1\n",
    "for value in sys.stdin:\n",
    "    value = int(value)\n",
    "    for i in range(data_count):\n",
    "        print(\"(%d,%d) %d\" % (counter, i+1, value))\n",
    "        print(\"(%d,%d) %d\" % (i+1, counter, value))\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cartesianReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file cartesianReducer.py\n",
    "#!/home/ubuntu/anaconda3/bin/python\n",
    "\"\"\"\n",
    "Reducer Kartesian\n",
    "Berikut adalah code pada reducer yang berfungsi mengkalikan data dengan key yang sama.\n",
    "\n",
    "<(1,1) data1> <(1,1) data2> menjadi <(1,1) data1 * data2> \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Analisa kode berikut untuk menambahkan pemahaman anda!\n",
    "Kode ini mirip seperti pada kasus wordcount.\n",
    "\"\"\"\n",
    "import sys\n",
    "current_key = None\n",
    "current_sum = 0\n",
    "for kv in sys.stdin:\n",
    "    key, value = kv.split(\" \")\n",
    "    value = int(value)\n",
    "    if current_key == key :\n",
    "        current_sum *= value\n",
    "    elif current_key != key:\n",
    "        if current_key is not None:\n",
    "            print(\"%s %d\" % (current_key, current_sum))\n",
    "        current_key = key\n",
    "        current_sum = value\n",
    "print(\"%s %d\" % (current_key, current_sum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod u+x counterReducer.py cartesianMapper.py cartesianReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bangkitkan data yang berisi angka 1 - 10\n",
    "count = 10\n",
    "with open(\"numbers.txt\", \"w\") as numbers:\n",
    "    for i in range(count):\n",
    "        numbers.write(\"%d\\n\" % (i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,1) 1\r\n",
      "(1,10) 10\r\n",
      "(1,2) 2\r\n",
      "(1,3) 3\r\n",
      "(1,4) 4\r\n",
      "(1,5) 5\r\n",
      "(1,6) 6\r\n",
      "(1,7) 7\r\n",
      "(1,8) 8\r\n",
      "(1,9) 9\r\n",
      "(10,1) 10\r\n",
      "(10,10) 100\r\n",
      "(10,2) 20\r\n",
      "(10,3) 30\r\n",
      "(10,4) 40\r\n",
      "(10,5) 50\r\n",
      "(10,6) 60\r\n",
      "(10,7) 70\r\n",
      "(10,8) 80\r\n",
      "(10,9) 90\r\n",
      "(2,1) 2\r\n",
      "(2,10) 20\r\n",
      "(2,2) 4\r\n",
      "(2,3) 6\r\n",
      "(2,4) 8\r\n",
      "(2,5) 10\r\n",
      "(2,6) 12\r\n",
      "(2,7) 14\r\n",
      "(2,8) 16\r\n",
      "(2,9) 18\r\n",
      "(3,1) 3\r\n",
      "(3,10) 30\r\n",
      "(3,2) 6\r\n",
      "(3,3) 9\r\n",
      "(3,4) 12\r\n",
      "(3,5) 15\r\n",
      "(3,6) 18\r\n",
      "(3,7) 21\r\n",
      "(3,8) 24\r\n",
      "(3,9) 27\r\n",
      "(4,1) 4\r\n",
      "(4,10) 40\r\n",
      "(4,2) 8\r\n",
      "(4,3) 12\r\n",
      "(4,4) 16\r\n",
      "(4,5) 20\r\n",
      "(4,6) 24\r\n",
      "(4,7) 28\r\n",
      "(4,8) 32\r\n",
      "(4,9) 36\r\n",
      "(5,1) 5\r\n",
      "(5,10) 50\r\n",
      "(5,2) 10\r\n",
      "(5,3) 15\r\n",
      "(5,4) 20\r\n",
      "(5,5) 25\r\n",
      "(5,6) 30\r\n",
      "(5,7) 35\r\n",
      "(5,8) 40\r\n",
      "(5,9) 45\r\n",
      "(6,1) 6\r\n",
      "(6,10) 60\r\n",
      "(6,2) 12\r\n",
      "(6,3) 18\r\n",
      "(6,4) 24\r\n",
      "(6,5) 30\r\n",
      "(6,6) 36\r\n",
      "(6,7) 42\r\n",
      "(6,8) 48\r\n",
      "(6,9) 54\r\n",
      "(7,1) 7\r\n",
      "(7,10) 70\r\n",
      "(7,2) 14\r\n",
      "(7,3) 21\r\n",
      "(7,4) 28\r\n",
      "(7,5) 35\r\n",
      "(7,6) 42\r\n",
      "(7,7) 49\r\n",
      "(7,8) 56\r\n",
      "(7,9) 63\r\n",
      "(8,1) 8\r\n",
      "(8,10) 80\r\n",
      "(8,2) 16\r\n",
      "(8,3) 24\r\n",
      "(8,4) 32\r\n",
      "(8,5) 40\r\n",
      "(8,6) 48\r\n",
      "(8,7) 56\r\n",
      "(8,8) 64\r\n",
      "(8,9) 72\r\n",
      "(9,1) 9\r\n",
      "(9,10) 90\r\n",
      "(9,2) 18\r\n",
      "(9,3) 27\r\n",
      "(9,4) 36\r\n",
      "(9,5) 45\r\n",
      "(9,6) 54\r\n",
      "(9,7) 63\r\n",
      "(9,8) 72\r\n",
      "(9,9) 81\r\n"
     ]
    }
   ],
   "source": [
    "#jalankan program menghitung banyak data. simpan ke file counter\n",
    "!cat numbers.txt | python counterMapper.py | sort |python counterReducer.py  > counter\n",
    "#jalankan program menghitung kartesian product\n",
    "!cat numbers.txt | python cartesianMapper.py | sort | python cartesianReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted input/cartesian\n",
      "Deleted output/cartesian\n"
     ]
    }
   ],
   "source": [
    "!rm counter\n",
    "!hdfs dfs -rm -r input/cartesian\n",
    "!hdfs dfs -rm -r output/cartesian\n",
    "!hdfs dfs -mkdir -p input/cartesian\n",
    "!hdfs dfs -put numbers.txt input/cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/06/29 05:37:48 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "19/06/29 05:37:48 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "19/06/29 05:37:48 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "19/06/29 05:37:48 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "19/06/29 05:37:48 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "19/06/29 05:37:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1258039002_0001\n",
      "19/06/29 05:37:49 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "19/06/29 05:37:49 INFO mapreduce.Job: Running job: job_local1258039002_0001\n",
      "19/06/29 05:37:49 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "19/06/29 05:37:49 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "19/06/29 05:37:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/06/29 05:37:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/06/29 05:37:49 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "19/06/29 05:37:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1258039002_0001_m_000000_0\n",
      "19/06/29 05:37:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/06/29 05:37:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/06/29 05:37:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/06/29 05:37:49 INFO mapred.MapTask: Processing split: hdfs://localhost:8020/user/ubuntu/input/cartesian/numbers.txt:0+21\n",
      "19/06/29 05:37:49 INFO mapred.MapTask: numReduceTasks: 1\n",
      "19/06/29 05:37:49 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "19/06/29 05:37:49 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "19/06/29 05:37:49 INFO mapred.MapTask: soft limit at 83886080\n",
      "19/06/29 05:37:49 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "19/06/29 05:37:49 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "19/06/29 05:37:49 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "19/06/29 05:37:49 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/notebook/cartesian/./counterMapper.py]\n",
      "19/06/29 05:37:49 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "19/06/29 05:37:49 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "19/06/29 05:37:49 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "19/06/29 05:37:49 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "19/06/29 05:37:49 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "19/06/29 05:37:49 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "19/06/29 05:37:49 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "19/06/29 05:37:49 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "19/06/29 05:37:49 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "19/06/29 05:37:49 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "19/06/29 05:37:49 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "19/06/29 05:37:49 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "19/06/29 05:37:49 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 05:37:49 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 05:37:49 INFO streaming.PipeMapRed: Records R/W=10/1\n",
      "19/06/29 05:37:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/06/29 05:37:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/06/29 05:37:49 INFO mapred.LocalJobRunner: \n",
      "19/06/29 05:37:49 INFO mapred.MapTask: Starting flush of map output\n",
      "19/06/29 05:37:49 INFO mapred.MapTask: Spilling map output\n",
      "19/06/29 05:37:49 INFO mapred.MapTask: bufstart = 0; bufend = 4; bufvoid = 104857600\n",
      "19/06/29 05:37:49 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
      "19/06/29 05:37:49 INFO mapred.MapTask: Finished spill 0\n",
      "19/06/29 05:37:49 INFO mapred.Task: Task:attempt_local1258039002_0001_m_000000_0 is done. And is in the process of committing\n",
      "19/06/29 05:37:49 INFO mapred.LocalJobRunner: Records R/W=10/1\n",
      "19/06/29 05:37:49 INFO mapred.Task: Task 'attempt_local1258039002_0001_m_000000_0' done.\n",
      "19/06/29 05:37:49 INFO mapred.Task: Final Counters for attempt_local1258039002_0001_m_000000_0: Counters: 22\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=134050\n",
      "\t\tFILE: Number of bytes written=499074\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=21\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=5\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=1\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=4\n",
      "\t\tMap output materialized bytes=12\n",
      "\t\tInput split bytes=113\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=33\n",
      "\t\tTotal committed heap usage (bytes)=121180160\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=21\n",
      "19/06/29 05:37:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1258039002_0001_m_000000_0\n",
      "19/06/29 05:37:49 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "19/06/29 05:37:49 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "19/06/29 05:37:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1258039002_0001_r_000000_0\n",
      "19/06/29 05:37:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/06/29 05:37:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/06/29 05:37:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/06/29 05:37:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@c3424a2\n",
      "19/06/29 05:37:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "19/06/29 05:37:49 INFO reduce.EventFetcher: attempt_local1258039002_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "19/06/29 05:37:50 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1258039002_0001_m_000000_0 decomp: 8 len: 12 to MEMORY\n",
      "19/06/29 05:37:50 INFO reduce.InMemoryMapOutput: Read 8 bytes from map-output for attempt_local1258039002_0001_m_000000_0\n",
      "19/06/29 05:37:50 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 8, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8\n",
      "19/06/29 05:37:50 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "19/06/29 05:37:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/06/29 05:37:50 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "19/06/29 05:37:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/06/29 05:37:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
      "19/06/29 05:37:50 INFO reduce.MergeManagerImpl: Merged 1 segments, 8 bytes to disk to satisfy reduce memory limit\n",
      "19/06/29 05:37:50 INFO reduce.MergeManagerImpl: Merging 1 files, 12 bytes from disk\n",
      "19/06/29 05:37:50 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "19/06/29 05:37:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/06/29 05:37:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
      "19/06/29 05:37:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/06/29 05:37:50 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/notebook/cartesian/./counterReducer.py]\n",
      "19/06/29 05:37:50 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/06/29 05:37:50 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/06/29 05:37:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 05:37:50 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "19/06/29 05:37:50 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/06/29 05:37:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/06/29 05:37:50 INFO mapreduce.Job: Job job_local1258039002_0001 running in uber mode : false\n",
      "19/06/29 05:37:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/06/29 05:37:50 INFO mapred.Task: Task:attempt_local1258039002_0001_r_000000_0 is done. And is in the process of committing\n",
      "19/06/29 05:37:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/06/29 05:37:50 INFO mapred.Task: Task attempt_local1258039002_0001_r_000000_0 is allowed to commit now\n",
      "19/06/29 05:37:50 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1258039002_0001_r_000000_0' to hdfs://localhost:8020/user/ubuntu/output/cartesian/tmp/_temporary/0/task_local1258039002_0001_r_000000\n",
      "19/06/29 05:37:50 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "19/06/29 05:37:50 INFO mapred.Task: Task 'attempt_local1258039002_0001_r_000000_0' done.\n",
      "19/06/29 05:37:50 INFO mapred.Task: Final Counters for attempt_local1258039002_0001_r_000000_0: Counters: 29\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=134106\n",
      "\t\tFILE: Number of bytes written=499086\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=21\n",
      "\t\tHDFS: Number of bytes written=4\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=12\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=1\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=7\n",
      "\t\tTotal committed heap usage (bytes)=121180160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4\n",
      "19/06/29 05:37:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local1258039002_0001_r_000000_0\n",
      "19/06/29 05:37:50 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "19/06/29 05:37:51 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/06/29 05:37:51 INFO mapreduce.Job: Job job_local1258039002_0001 completed successfully\n",
      "19/06/29 05:37:51 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=268156\n",
      "\t\tFILE: Number of bytes written=998160\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=42\n",
      "\t\tHDFS: Number of bytes written=4\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=4\n",
      "\t\tMap output materialized bytes=12\n",
      "\t\tInput split bytes=113\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=12\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=40\n",
      "\t\tTotal committed heap usage (bytes)=242360320\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=21\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4\n",
      "19/06/29 05:37:51 INFO streaming.StreamJob: Output directory: output/cartesian/tmp\n",
      "19/06/29 05:37:59 WARN streaming.StreamJob: -cacheFile option is deprecated, please use -files instead.\n",
      "19/06/29 05:38:01 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "19/06/29 05:38:01 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "19/06/29 05:38:01 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "19/06/29 05:38:01 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "19/06/29 05:38:01 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "19/06/29 05:38:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local933574709_0001\n",
      "19/06/29 05:38:02 INFO mapred.LocalDistributedCacheManager: Localized hdfs://localhost:8020/user/ubuntu/output/cartesian/tmp/counter as file:/tmp/mapred/local/1561786682236/counter\n",
      "19/06/29 05:38:02 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "19/06/29 05:38:02 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "19/06/29 05:38:02 INFO mapreduce.Job: Running job: job_local933574709_0001\n",
      "19/06/29 05:38:02 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "19/06/29 05:38:02 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/06/29 05:38:02 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/06/29 05:38:02 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "19/06/29 05:38:02 INFO mapred.LocalJobRunner: Starting task: attempt_local933574709_0001_m_000000_0\n",
      "19/06/29 05:38:02 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/06/29 05:38:02 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/06/29 05:38:02 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/06/29 05:38:02 INFO mapred.MapTask: Processing split: hdfs://localhost:8020/user/ubuntu/input/cartesian/numbers.txt:0+21\n",
      "19/06/29 05:38:02 INFO mapred.MapTask: numReduceTasks: 1\n",
      "19/06/29 05:38:03 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "19/06/29 05:38:03 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "19/06/29 05:38:03 INFO mapred.MapTask: soft limit at 83886080\n",
      "19/06/29 05:38:03 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "19/06/29 05:38:03 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "19/06/29 05:38:03 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "19/06/29 05:38:03 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/notebook/cartesian/./cartesianMapper.py]\n",
      "19/06/29 05:38:03 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "19/06/29 05:38:03 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "19/06/29 05:38:03 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "19/06/29 05:38:03 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "19/06/29 05:38:03 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "19/06/29 05:38:03 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "19/06/29 05:38:03 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "19/06/29 05:38:03 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "19/06/29 05:38:03 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "19/06/29 05:38:03 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "19/06/29 05:38:03 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "19/06/29 05:38:03 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "19/06/29 05:38:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 05:38:03 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 05:38:03 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/06/29 05:38:03 INFO streaming.PipeMapRed: Records R/W=10/1\n",
      "19/06/29 05:38:03 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/06/29 05:38:03 INFO mapred.LocalJobRunner: \n",
      "19/06/29 05:38:03 INFO mapred.MapTask: Starting flush of map output\n",
      "19/06/29 05:38:03 INFO mapred.MapTask: Spilling map output\n",
      "19/06/29 05:38:03 INFO mapred.MapTask: bufstart = 0; bufend = 1860; bufvoid = 104857600\n",
      "19/06/29 05:38:03 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213600(104854400); length = 797/6553600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/06/29 05:38:03 INFO mapred.MapTask: Finished spill 0\n",
      "19/06/29 05:38:03 INFO mapred.Task: Task:attempt_local933574709_0001_m_000000_0 is done. And is in the process of committing\n",
      "19/06/29 05:38:03 INFO mapred.LocalJobRunner: Records R/W=10/1\n",
      "19/06/29 05:38:03 INFO mapred.Task: Task 'attempt_local933574709_0001_m_000000_0' done.\n",
      "19/06/29 05:38:03 INFO mapred.Task: Final Counters for attempt_local933574709_0001_m_000000_0: Counters: 22\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=134050\n",
      "\t\tFILE: Number of bytes written=501319\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=25\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=22\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=1\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10\n",
      "\t\tMap output records=200\n",
      "\t\tMap output bytes=1860\n",
      "\t\tMap output materialized bytes=2266\n",
      "\t\tInput split bytes=113\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=200\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=51\n",
      "\t\tTotal committed heap usage (bytes)=121753600\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=21\n",
      "19/06/29 05:38:03 INFO mapred.LocalJobRunner: Finishing task: attempt_local933574709_0001_m_000000_0\n",
      "19/06/29 05:38:03 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "19/06/29 05:38:03 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "19/06/29 05:38:03 INFO mapred.LocalJobRunner: Starting task: attempt_local933574709_0001_r_000000_0\n",
      "19/06/29 05:38:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/06/29 05:38:03 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/06/29 05:38:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/06/29 05:38:03 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@609f11b8\n",
      "19/06/29 05:38:03 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "19/06/29 05:38:03 INFO reduce.EventFetcher: attempt_local933574709_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "19/06/29 05:38:03 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local933574709_0001_m_000000_0 decomp: 2262 len: 2266 to MEMORY\n",
      "19/06/29 05:38:03 INFO reduce.InMemoryMapOutput: Read 2262 bytes from map-output for attempt_local933574709_0001_m_000000_0\n",
      "19/06/29 05:38:03 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2262, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2262\n",
      "19/06/29 05:38:03 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "19/06/29 05:38:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/06/29 05:38:03 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "19/06/29 05:38:03 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/06/29 05:38:03 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2252 bytes\n",
      "19/06/29 05:38:03 INFO reduce.MergeManagerImpl: Merged 1 segments, 2262 bytes to disk to satisfy reduce memory limit\n",
      "19/06/29 05:38:03 INFO reduce.MergeManagerImpl: Merging 1 files, 2266 bytes from disk\n",
      "19/06/29 05:38:03 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "19/06/29 05:38:03 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/06/29 05:38:03 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2252 bytes\n",
      "19/06/29 05:38:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/06/29 05:38:03 INFO mapreduce.Job: Job job_local933574709_0001 running in uber mode : false\n",
      "19/06/29 05:38:03 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/notebook/cartesian/./cartesianReducer.py]\n",
      "19/06/29 05:38:03 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/06/29 05:38:03 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/06/29 05:38:03 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "19/06/29 05:38:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 05:38:03 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 05:38:03 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/06/29 05:38:03 INFO streaming.PipeMapRed: Records R/W=200/1\n",
      "19/06/29 05:38:03 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/06/29 05:38:03 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/06/29 05:38:03 INFO mapred.Task: Task:attempt_local933574709_0001_r_000000_0 is done. And is in the process of committing\n",
      "19/06/29 05:38:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/06/29 05:38:03 INFO mapred.Task: Task attempt_local933574709_0001_r_000000_0 is allowed to commit now\n",
      "19/06/29 05:38:03 INFO output.FileOutputCommitter: Saved output of task 'attempt_local933574709_0001_r_000000_0' to hdfs://localhost:8020/user/ubuntu/output/cartesian/result/_temporary/0/task_local933574709_0001_r_000000\n",
      "19/06/29 05:38:03 INFO mapred.LocalJobRunner: Records R/W=200/1 > reduce\n",
      "19/06/29 05:38:03 INFO mapred.Task: Task 'attempt_local933574709_0001_r_000000_0' done.\n",
      "19/06/29 05:38:03 INFO mapred.Task: Final Counters for attempt_local933574709_0001_r_000000_0: Counters: 29\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=138614\n",
      "\t\tFILE: Number of bytes written=503585\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=25\n",
      "\t\tHDFS: Number of bytes written=998\n",
      "\t\tHDFS: Number of read operations=25\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=190\n",
      "\t\tReduce shuffle bytes=2266\n",
      "\t\tReduce input records=200\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=8\n",
      "\t\tTotal committed heap usage (bytes)=121753600\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=998\n",
      "19/06/29 05:38:03 INFO mapred.LocalJobRunner: Finishing task: attempt_local933574709_0001_r_000000_0\n",
      "19/06/29 05:38:03 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "19/06/29 05:38:04 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/06/29 05:38:04 INFO mapreduce.Job: Job job_local933574709_0001 completed successfully\n",
      "19/06/29 05:38:04 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=272664\n",
      "\t\tFILE: Number of bytes written=1004904\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50\n",
      "\t\tHDFS: Number of bytes written=998\n",
      "\t\tHDFS: Number of read operations=47\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10\n",
      "\t\tMap output records=200\n",
      "\t\tMap output bytes=1860\n",
      "\t\tMap output materialized bytes=2266\n",
      "\t\tInput split bytes=113\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=190\n",
      "\t\tReduce shuffle bytes=2266\n",
      "\t\tReduce input records=200\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=400\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=59\n",
      "\t\tTotal committed heap usage (bytes)=243507200\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=21\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=998\n",
      "19/06/29 05:38:04 INFO streaming.StreamJob: Output directory: output/cartesian/result\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /home/ubuntu/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar \\\n",
    "-mapper counterMapper.py \\\n",
    "-reducer counterReducer.py \\\n",
    "-input input/cartesian/numbers.txt \\\n",
    "-output output/cartesian/tmp\n",
    "\n",
    "#gabung hasil reducer program menghitung banyak data (sebenarnya tidak diperlukan hanya perlu diunduh saja pada master)\n",
    "!hdfs dfs -getmerge output/cartesian/tmp/* counter\n",
    "#masukkan hasil perhitungan banyak data ke HDFS\n",
    "!hdfs dfs -put counter output/cartesian/tmp/counter\n",
    "\n",
    "!hadoop jar /home/ubuntu/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar \\\n",
    "-mapper cartesianMapper.py \\\n",
    "-reducer cartesianReducer.py \\\n",
    "-input input/cartesian/numbers.txt \\\n",
    "-output output/cartesian/result \\\n",
    "-cacheFile \"output/cartesian/tmp/counter#counter\"\n",
    "\n",
    "#perhatikan perintah Hadoop diatas!\n",
    "#fungsi parameter -cacheFile adalah memberitahu program terdapat sebuah cache file yang dapat dibuka.\n",
    "#cache file adalah file berukuran kecil yang berisi konfigurasi atau data berukuran kecil lain yang digunakan -\n",
    "#untuk menunjang kinerja MapReduce.\n",
    "\n",
    "#cache file disarankan berukuran se-KECIL mungkin karena cara kerja cache file yang disalin ke masing-masing node -\n",
    "#sehingga dapat mengurangi kinerja MapReduce.\n",
    "\n",
    "#Dalam kasus ini cacheFile berisi banyak data yang telah dihitung sebelumnya dan hanya berukuran beberapa KB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,1) 1\t\r\n",
      "(1,10) 10\t\r\n",
      "(1,2) 2\t\r\n",
      "(1,3) 3\t\r\n",
      "(1,4) 4\t\r\n",
      "(1,5) 5\t\r\n",
      "(1,6) 6\t\r\n",
      "(1,7) 7\t\r\n",
      "(1,8) 8\t\r\n",
      "(1,9) 9\t\r\n",
      "(10,1) 10\t\r\n",
      "(10,10) 100\t\r\n",
      "(10,2) 20\t\r\n",
      "(10,3) 30\t\r\n",
      "(10,4) 40\t\r\n",
      "(10,5) 50\t\r\n",
      "(10,6) 60\t\r\n",
      "(10,7) 70\t\r\n",
      "(10,8) 80\t\r\n",
      "(10,9) 90\t\r\n",
      "(2,1) 2\t\r\n",
      "(2,10) 20\t\r\n",
      "(2,2) 4\t\r\n",
      "(2,3) 6\t\r\n",
      "(2,4) 8\t\r\n",
      "(2,5) 10\t\r\n",
      "(2,6) 12\t\r\n",
      "(2,7) 14\t\r\n",
      "(2,8) 16\t\r\n",
      "(2,9) 18\t\r\n",
      "(3,1) 3\t\r\n",
      "(3,10) 30\t\r\n",
      "(3,2) 6\t\r\n",
      "(3,3) 9\t\r\n",
      "(3,4) 12\t\r\n",
      "(3,5) 15\t\r\n",
      "(3,6) 18\t\r\n",
      "(3,7) 21\t\r\n",
      "(3,8) 24\t\r\n",
      "(3,9) 27\t\r\n",
      "(4,1) 4\t\r\n",
      "(4,10) 40\t\r\n",
      "(4,2) 8\t\r\n",
      "(4,3) 12\t\r\n",
      "(4,4) 16\t\r\n",
      "(4,5) 20\t\r\n",
      "(4,6) 24\t\r\n",
      "(4,7) 28\t\r\n",
      "(4,8) 32\t\r\n",
      "(4,9) 36\t\r\n",
      "(5,1) 5\t\r\n",
      "(5,10) 50\t\r\n",
      "(5,2) 10\t\r\n",
      "(5,3) 15\t\r\n",
      "(5,4) 20\t\r\n",
      "(5,5) 25\t\r\n",
      "(5,6) 30\t\r\n",
      "(5,7) 35\t\r\n",
      "(5,8) 40\t\r\n",
      "(5,9) 45\t\r\n",
      "(6,1) 6\t\r\n",
      "(6,10) 60\t\r\n",
      "(6,2) 12\t\r\n",
      "(6,3) 18\t\r\n",
      "(6,4) 24\t\r\n",
      "(6,5) 30\t\r\n",
      "(6,6) 36\t\r\n",
      "(6,7) 42\t\r\n",
      "(6,8) 48\t\r\n",
      "(6,9) 54\t\r\n",
      "(7,1) 7\t\r\n",
      "(7,10) 70\t\r\n",
      "(7,2) 14\t\r\n",
      "(7,3) 21\t\r\n",
      "(7,4) 28\t\r\n",
      "(7,5) 35\t\r\n",
      "(7,6) 42\t\r\n",
      "(7,7) 49\t\r\n",
      "(7,8) 56\t\r\n",
      "(7,9) 63\t\r\n",
      "(8,1) 8\t\r\n",
      "(8,10) 80\t\r\n",
      "(8,2) 16\t\r\n",
      "(8,3) 24\t\r\n",
      "(8,4) 32\t\r\n",
      "(8,5) 40\t\r\n",
      "(8,6) 48\t\r\n",
      "(8,7) 56\t\r\n",
      "(8,8) 64\t\r\n",
      "(8,9) 72\t\r\n",
      "(9,1) 9\t\r\n",
      "(9,10) 90\t\r\n",
      "(9,2) 18\t\r\n",
      "(9,3) 27\t\r\n",
      "(9,4) 36\t\r\n",
      "(9,5) 45\t\r\n",
      "(9,6) 54\t\r\n",
      "(9,7) 63\t\r\n",
      "(9,8) 72\t\r\n",
      "(9,9) 81\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat output/cartesian/result/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cobalah ganti jumlah data menjadi sebanyak mungkin (jangan terlalu banyak) dan amati lama eksekusi program!!</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
